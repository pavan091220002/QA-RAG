{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import fitz\n",
        "from PIL import Image\n",
        "import io\n",
        "import base64\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "import tempfile\n",
        "import torch"
      ],
      "metadata": {
        "id": "QqWGc_fu1hjE"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "df1DkHdL1lq5",
        "outputId": "48297027-ad4f-4209-b0c9-3515f3927550"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embedder = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-base\")\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-base\", device_map=\"auto\", torch_dtype=torch.float16)"
      ],
      "metadata": {
        "id": "mOMkJKhN1phY"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_content_from_pdf(pdf_path, output_dir='extracted_images'):\n",
        "    \"\"\"\n",
        "    Extracts text and images from a PDF file.\n",
        "\n",
        "    Args:\n",
        "        pdf_path (str): Path to the PDF file\n",
        "        output_dir (str): Directory to save extracted images\n",
        "\n",
        "    Returns:\n",
        "        Tuple[List[Dict], List[Dict]]: Text data and image data\n",
        "    \"\"\"\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    text_data = []\n",
        "    image_paths = []\n",
        "\n",
        "    with fitz.open(pdf_path) as pdf:\n",
        "        for page_num in range(len(pdf)):\n",
        "            page = pdf[page_num]\n",
        "            text = page.get_text().strip()\n",
        "            if text:\n",
        "                text_data.append({\n",
        "                    'content': text,\n",
        "                    'metadata': {'source': pdf_path, 'page': page_num + 1, 'type': 'text'}\n",
        "                })\n",
        "\n",
        "            image_list = page.get_images(full=True)\n",
        "            for img_index, img in enumerate(image_list):\n",
        "                xref = img[0]\n",
        "                base_image = pdf.extract_image(xref)\n",
        "                if base_image:\n",
        "                    image_bytes = base_image['image']\n",
        "                    image_ext = base_image['ext']\n",
        "                    img_filename = f\"page_{page_num+1}_img_{img_index+1}.{image_ext}\"\n",
        "                    img_path = os.path.join(output_dir, img_filename)\n",
        "                    with open(img_path, 'wb') as img_file:\n",
        "                        img_file.write(image_bytes)\n",
        "                    image_paths.append({\n",
        "                        'path': img_path,\n",
        "                        'metadata': {'source': pdf_path, 'page': page_num + 1, 'image_index': img_index + 1, 'type': 'image'}\n",
        "                    })\n",
        "\n",
        "    return text_data, image_paths"
      ],
      "metadata": {
        "id": "vqjT6KQ81w30"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def semantic_chunking(text_data, chunk_size=500, overlap=50, percentile_threshold=85):\n",
        "    \"\"\"\n",
        "    Splits text into semantic chunks with overlap.\n",
        "\n",
        "    Args:\n",
        "        text_data (List[Dict]): Extracted text data\n",
        "        chunk_size (int): Approximate size of each chunk in characters\n",
        "        overlap (int): Number of characters to overlap between chunks\n",
        "        percentile_threshold (float): Percentile for similarity breakpoint\n",
        "\n",
        "    Returns:\n",
        "        List[Dict]: Chunked text data\n",
        "    \"\"\"\n",
        "    chunked_data = []\n",
        "\n",
        "    for item in text_data:\n",
        "        text = item['content']\n",
        "        metadata = item['metadata']\n",
        "\n",
        "        sentences = sent_tokenize(text)\n",
        "        if len(sentences) < 2:\n",
        "            chunked_data.append({'content': text, 'metadata': metadata})\n",
        "            continue\n",
        "\n",
        "        sentence_embeddings = embedder.encode(sentences)\n",
        "        similarities = [\n",
        "            np.dot(sentence_embeddings[i], sentence_embeddings[i+1]) /\n",
        "            (np.linalg.norm(sentence_embeddings[i]) * np.linalg.norm(sentence_embeddings[i+1]))\n",
        "            for i in range(len(sentence_embeddings)-1)\n",
        "        ]\n",
        "\n",
        "        threshold = np.percentile(similarities, percentile_threshold)\n",
        "        breakpoints = [i for i, sim in enumerate(similarities) if sim < threshold]\n",
        "\n",
        "        start = 0\n",
        "        current_chunk = []\n",
        "        current_length = 0\n",
        "\n",
        "        for i, sentence in enumerate(sentences):\n",
        "            current_chunk.append(sentence)\n",
        "            current_length += len(sentence)\n",
        "\n",
        "            if i in breakpoints or current_length >= chunk_size:\n",
        "                chunk_text = ' '.join(current_chunk)\n",
        "                if len(chunk_text) > chunk_size // 2:\n",
        "                    chunked_data.append({\n",
        "                        'content': chunk_text,\n",
        "                        'metadata': {**metadata, 'chunk_index': len(chunked_data)}\n",
        "                    })\n",
        "\n",
        "                # Handle overlap\n",
        "                overlap_sentences = []\n",
        "                overlap_length = 0\n",
        "                for s in current_chunk[::-1]:\n",
        "                    if overlap_length + len(s) <= overlap:\n",
        "                        overlap_sentences.insert(0, s)\n",
        "                        overlap_length += len(s)\n",
        "                    else:\n",
        "                        break\n",
        "                current_chunk = overlap_sentences\n",
        "                current_length = overlap_length\n",
        "\n",
        "        if current_chunk:\n",
        "            chunk_text = ' '.join(current_chunk)\n",
        "            if len(chunk_text) > chunk_size // 2:\n",
        "                chunked_data.append({\n",
        "                    'content': chunk_text,\n",
        "                    'metadata': {**metadata, 'chunk_index': len(chunked_data)}\n",
        "                })\n",
        "    return chunked_data"
      ],
      "metadata": {
        "id": "OaUF7hgf2DLV"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_image_caption(image_path):\n",
        "    \"\"\"\n",
        "    Generates a caption for an image using a simple description (placeholder).\n",
        "\n",
        "    Args:\n",
        "        image_path (str): Path to the image file\n",
        "\n",
        "    Returns:\n",
        "        str: Generated caption\n",
        "    \"\"\"\n",
        "    try:\n",
        "        img = Image.open(image_path)\n",
        "        return f\"Image from page {os.path.basename(image_path).split('_')[1]} containing academic content, likely a chart or diagram.\"\n",
        "    except Exception as e:\n",
        "        return f\"Error generating caption: {str(e)}\""
      ],
      "metadata": {
        "id": "NBCGrb6x2Khf"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_images(image_paths):\n",
        "    \"\"\"\n",
        "    Processes images and generates captions.\n",
        "\n",
        "    Args:\n",
        "        image_paths (List[Dict]): Paths to extracted images\n",
        "\n",
        "    Returns:\n",
        "        List[Dict]: Image data with captions\n",
        "    \"\"\"\n",
        "    image_data = []\n",
        "    for img_item in image_paths:\n",
        "        caption = generate_image_caption(img_item['path'])\n",
        "        image_data.append({\n",
        "            'content': caption,\n",
        "            'metadata': img_item['metadata'],\n",
        "            'image_path': img_item['path']\n",
        "        })\n",
        "    return image_data"
      ],
      "metadata": {
        "id": "HH9jYAfW2M6g"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VectorStore:\n",
        "    def __init__(self):\n",
        "        self.index = None\n",
        "        self.contents = []\n",
        "        self.metadata = []\n",
        "\n",
        "    def add_items(self, items, embeddings):\n",
        "        \"\"\"\n",
        "        Adds items and their embeddings to the FAISS index.\n",
        "\n",
        "        Args:\n",
        "            items (List[Dict]): Content items\n",
        "            embeddings (List[np.ndarray]): Corresponding embeddings\n",
        "        \"\"\"\n",
        "        dimension = embeddings[0].shape[0]\n",
        "        self.index = faiss.IndexFlatL2(dimension)\n",
        "        self.index.add(np.array(embeddings).astype('float32'))\n",
        "        self.contents = [item['content'] for item in items]\n",
        "        self.metadata = [item.get('metadata', {}) for item in items]\n",
        "\n",
        "    def search(self, query_embedding, k=5):\n",
        "        \"\"\"\n",
        "        Performs similarity search in the vector store.\n",
        "\n",
        "        Args:\n",
        "            query_embedding (np.ndarray): Query embedding\n",
        "            k (int): Number of results to return\n",
        "\n",
        "        Returns:\n",
        "            List[Dict]: Top-k similar items\n",
        "        \"\"\"\n",
        "        if self.index is None:\n",
        "            return []\n",
        "\n",
        "        distances, indices = self.index.search(np.array([query_embedding]).astype('float32'), k)\n",
        "        results = []\n",
        "        for idx, distance in zip(indices[0], distances[0]):\n",
        "            results.append({\n",
        "                'content': self.contents[idx],\n",
        "                'metadata': self.metadata[idx],\n",
        "                'similarity': 1 / (1 + distance)\n",
        "            })\n",
        "        return results"
      ],
      "metadata": {
        "id": "YB0zXxxD2VHu"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_document(pdf_path, chunk_size=500, percentile_threshold=90):\n",
        "    \"\"\"\n",
        "    Processes a PDF document for RAG.\n",
        "\n",
        "    Args:\n",
        "        pdf_path (str): Path to the PDF\n",
        "        chunk_size (int): Approximate chunk size\n",
        "        percentile_threshold (float): Percentile for semantic chunking\n",
        "\n",
        "    Returns:\n",
        "        Tuple[VectorStore, Dict]: Vector store and document info\n",
        "    \"\"\"\n",
        "    text_data, image_paths = extract_content_from_pdf(pdf_path)\n",
        "    chunked_text = semantic_chunking(text_data, chunk_size, percentile_threshold)\n",
        "    image_data = process_images(image_paths)\n",
        "\n",
        "    all_items = chunked_text + image_data\n",
        "    embeddings = embedder.encode([item['content'] for item in all_items])\n",
        "\n",
        "    vector_store = VectorStore()\n",
        "    vector_store.add_items(all_items, embeddings)\n",
        "\n",
        "    doc_info = {\n",
        "        'text_count': len(chunked_text),\n",
        "        'image_count': len(image_data),\n",
        "        'total_items': len(all_items)\n",
        "    }\n",
        "\n",
        "    return vector_store, doc_info"
      ],
      "metadata": {
        "id": "8P_MEBDZ2acO"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def query_rag(query, vector_store, k=5):\n",
        "    \"\"\"\n",
        "    Processes a query using the RAG system.\n",
        "\n",
        "    Args:\n",
        "        query (str): User query\n",
        "        vector_store (VectorStore): Vector store with document content\n",
        "        k (int): Number of results to retrieve\n",
        "\n",
        "    Returns:\n",
        "        Dict: Query results and generated response\n",
        "    \"\"\"\n",
        "    query_embedding = embedder.encode([query])[0]\n",
        "    results = vector_store.search(query_embedding, k)\n",
        "\n",
        "    context = '\\n\\n'.join(\n",
        "        f\"[{'Text' if r['metadata']['type'] == 'text' else 'Image Caption'} from page {r['metadata'].get('page', 'unknown')}]:\\n{r['content']}\"\n",
        "        for r in results\n",
        "    )\n",
        "\n",
        "    prompt = f\"\"\"You are an AI assistant specializing in answering questions based on clinical and mental health documents. Use the provided context to answer the query accurately. If the context is insufficient, state so clearly.\n",
        "Query: {query}\n",
        "Context:\n",
        "{context}\n",
        "Answer: \"\"\"\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", max_length=512, truncation=True).to(model.device)\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_length=200,\n",
        "        num_beams=4,\n",
        "        temperature=0.1,\n",
        "        do_sample=True,\n",
        "        pad_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    response = response.split('Answer:')[-1].strip() if 'Answer:' in response else response.strip()\n",
        "\n",
        "    return {\n",
        "        #'query': query,\n",
        "        'response': response,\n",
        "        #'results': results,\n",
        "        #'text_results_count': len([r for r in results if r['metadata']['type'] == 'text']),\n",
        "        #'image_results_count': len([r for r in results if r['metadata']['type'] == 'image'])\n",
        "    }"
      ],
      "metadata": {
        "id": "kuBTppvK2gv5"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pdf_path = \"9241544228_eng.pdf\"\n",
        "vector_store, doc_info = process_document(pdf_path)\n",
        "\n",
        "query = \"Give me the correct coded classification for the following diagnosis: Recurrent depressive disorder, currently in remission\"\n",
        "result = query_rag(query, vector_store)\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ug7xYVt13ayi",
        "outputId": "fe981279-fce0-472e-b3f3-d6b79ec33ad4"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'response': 'Recurrent depressive disorder, currently in remission'}\n"
          ]
        }
      ]
    }
  ]
}